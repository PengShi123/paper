# Sequence to Sequence Learning with Neural Networks
## Abstract
* DNN不能用于将序列映射到序列。
* 使用多层长短期存储器（LSTM）将输入序列映射到固定维度的向量。
* 另一个深度LSTM解码来自矩阵的目标序列。
* 论文的结果主要是：从英语到法语的翻译任务，在WNT14数据集上。
* LSTM在长句上的处理没有难度。
* 结论:
  * 当我们使用LSTM对上述SMT系统产生的1000个假设进行重新排序时，其BLEU得分增加到36.5，这接近于该任务之前的最佳结果。
  * LSTM还学习了对语序敏感、对主动语态和被动语态相对不变的合理短语和句子表示。
## 1、Introduction
* DNN可以实现任意并行计算。
* DNN只使用2个二次方大小的隐藏层就可以对N位数字进行排序。
* DNN只能应用于输入和目标可以用固定维度的向量编码的问题。
* 在问答问题中，可以看作是将表示问题的单词序列映射为表示答案的单词序列。
* 对于DNN来说输入和输出的维度是已知和固定的。
* 作者的想法是使用两个LSTM,先使用一个LSTM去阅读输入序列，去获得最大的固定的维度向量的表示。使用另一个LSTM从矩阵中去提取输出序列
* second LSTM是采用一个RNN
* LSTM之所以能够被广泛的选择是因为它能够处理长时间依赖的任务
* 在文章所提到的模型中：
  * 模型在输出句尾标记后停止预测
  * LSTM是反向读取输入句子，因为这样做会在数据中引入许多短期依赖关系，从而使优化问题变得更加容易。
* 在任务中作者采用简单的从左到右波束搜索解码器的5个深度LSTM的集合（每个具有384M个参数和8000维状态）
* 在进行BLEU的测试时每当翻译到词汇表中没有的词语，BLEU的分数就会降低。
* 所以具有很大改进空间的相对未优化的小词汇神经网络架构优于基于短语的SMT系统。
* 作者使用LSTM在同一任务中重新存储了公开可用的1000个SMT基线最佳列表。
* 模型能够实现那么好的效果是因为能够在长句上做得很好，因为在训练和测试集中颠倒了源句中的单词顺序，但没有颠倒目标句的顺序。
* SGD可以学习没有长句问题的LSTM
* LSTM可以将可变长度的输入句子映射到固定维的向量表示中
* 意思相近的句子彼此相近，而不同的两个句子的意思会相差很远。
## 2 The model
* 只要输入和输出之间的对齐提前已知，RNN就可以容易地将序列映射到序列。
* 目前的问题是：如何将RNN应用于输入和输出序列具有不同长度且具有复杂和非单调关系的问题。
* 一个RNN将输入的句子放到固定的矩阵中，将向量映射到目标序列利用另一个RNN
* 先计算v，v是通过LSTM最后一个隐藏层的状态，将v算作输出序列的第一个元素
* 每个句子的结尾放置一个<EOS>，使模型能够知道所有句子的长度
* 模型实现的三种重要方式：
  * 1、使用两种不同的LSTM，一种用在输入序列一种用在输出序列，可以忽略成本的额添加参数，同时在多个语言对上训练LSTM变得很自然
  * 2、深层LSTM效果比浅层LSTM效果好
  * 3、逆置输入句子的单词是非常有价值的
## 3 Experiments
### 3.1 Dataset details
* 模型是训练在由 348M 法语单词和 304M 英语单词组成的 12M 个句子的子集上
* 选择这个数据集的原因：这个翻译任务和这个特定的训练集子集，因为标记化的训练和测试集以及来自基线SMT的1000个最佳列表是公开的
* 数据集中作者为源语言使用了160000个最频繁的单词，为目标语言使用了80000个最频繁单词。
### 3.2 Decoding and Rescoring
* 论文的实验核心是在许多句子对上训练一个大型深层LSTM,一旦训练完成，根据LSTM找到最有可能翻译结果
### 3.3 Reversing the Source Sentences
* 通过颠倒源句子中的单词，源语言和目标语言中对应单词之间的平均距离不变。源语言中的前几个单词现在与目标语言中的头几个单词非常接近，因此大大减少了问题的最小时滞
* LSTM训练逆置的源语句可以获得更好的效果
### 3.4 Training details
* deep LSTM是由4个隐藏层组成每个隐藏层有1000个神经元和维度为1000的词编码器，用8000个真实的数字表示一个句子
* 在每个输出上使用超过 80,000个单词的朴素softmax，得到的LSTM具有384M个参数，其中64M是纯递归连接
* 训练细节：
  * 用 -0.08 和 0.08 之间的均匀分布初始化所有 LSTM 的参数
  * 使用了没有动量的随机梯度下降，固定学习率为 0.7，在 5 个 epoch 之后，我们开始每个epoch将学习率减半。对模型进行了总共 7.5 个 epoch 的训练
  * 使用128个序列的批次作为梯度，并将其划分为批次的大小。
  * 为了防止梯度爆炸，对梯度的范数施加了一个严格的约束，通过在其范数超过阈值时对其进行缩放。
  * 确保小批量中的所有句子大致相同的长度，从而产生 2 倍的加速。
### 3.5 Parallelization
* 我们使用8-GPU机器对模型进行了并行化。LSTM的每一层都在不同的GPU上执行，并在计算出激活后立即将其传递给下一个GPU/层。
* 每个GPU进行1000x2000的矩阵运算
### 3.6 Experimental Results
* 论文利用multi-bleu.pl计算BLEU
* 最佳结果是用LSTM的集合获得的，这些LSTM在随机初始化和小批量的随机顺序方面不同。
* 纯神经翻译系统首次优于基于短语的SMT基线
* 作者认为模型的一个吸引人的特点是它能够将单词序列转换为固定维度的向量
## 5 Conclusion
* 几乎没有假设问题结构可以优于基于SMT的标准系统，该系统的词汇在大规模MT任务中是无限的。
* 它应该在许多其他序列学习问题上做得很好，前提是它们有足够的训练数据。
* 得出的结论是，找到一个短期依赖性最大的编码问题是很重要的。
* 然而，在反向数据集上训练的LSTM在翻译长句方面几乎没有困难。