# Abstract
* 将自监督批量对比方法扩展到完全监督的形式，使我们能够更有效地利用标签信息。
* 属于同一类的点簇在嵌入空间中被拉到一起，同时推开来自不同类的样本簇。
# 1 Introduction
* 交叉熵损失是深度分类模型监督学习中使用最广泛的损失函数。许多工作已经探索了这种损失的缺点，例如对噪声标签缺乏鲁棒性和较差的裕度的可能性，导致泛化性能降低。
* 这些工作中的共同理念如下：
	* 将锚和嵌入空间中的“正”样本拉在一起，并将锚从许多“负”样本中推开。由于没有可用的标签，正对通常由样本的数据增强组成，负对由锚和从迷你批次中随机选择的样本组成。
* 在这项工作中，提出了监督学习的损失，该损失建立在通过利用标签信息进行对比的自我监督文献的基础上。来自同一类的规范化嵌入比来自不同类的嵌入更紧密地结合在一起。
* 这项工作中的技术创新是，除了许多消极因素外，每个锚点还考虑了许多积极因素（而不是只使用单个积极因素的自我监督对比学习）。
* 这些阳性是从与锚点相同类别的样本中提取的，而不是像在自我监督学习中那样是锚点的数据增强。
* 损失可以看作是三元组和N对损失的推广；前者每个锚只使用一个阳性和一个阴性样本，而后者使用一个正样本和多个阴性样本。每个锚点使用许多积极因素和许多消极因素，使我们能够在不需要硬消极挖掘的情况下实现最先进的性能，而硬消极挖掘可能很难正确调整。
* contributions：
	* 对对比损失函数提出了一种新的扩展，允许每个锚点有多个阳性，从而使对比学习适应完全监督的环境。从分析和实证的角度来看，我们证明了naıve扩展的性能比我们提出的版本差得多。
	* contrastive loss为许多数据集的前1名准确性提供了持续的提升。它对自然腐蚀也更具鲁棒性。
	* 损失函数的梯度鼓励从硬正和硬负中学习。
	*  contrastive loss对一系列超参数的敏感性不如交叉熵。
# 2 Related Work
* cross-entropy loss关键思想简单而直观:为每个类分配一个目标向量(通常是1-hot)。然而，目前还不清楚为什么这些目标标签应该是最优的，一些工作已经试图确定更好的目标标签向量。
* 已经提出了替代损失，但在实践中最有效的想法是改变参考标签分布的方法，如标签平滑，数据增强，如Mixup和CutMix，以及knowledge distillation。
* 前一层的嵌入用于下游传输任务、微调或直接检索任务.
* 与对比学习密切相关的是基于度量的距离学习或三元组损失。这些损失被用来学习强大的表征，通常在监督设置中，标签被用来指导正负对的选择。三元组损耗和对比损耗的关键区别在于每个数据点的正对和负对的数量;三元组损耗每个锚点恰好使用一对正极和一对负极。在监督度量学习设置中，正对从同一类中选择，负对从其他类中选择，几乎总是需要硬负挖掘才能获得良好的性能。同样，自监督对比损失对每个锚点样本只使用一个正对，选择方法是共现或数据增强，主要的区别是每个锚使用了许多负对，这些通常是使用某种形式的弱知识随机选择的，例如来自其他图像的补丁，或来自其他随机选择的视频的帧，依赖于这种方法产生假阴性概率非常低的假设。
* CCLP：基于标签传播的紧凑聚类
	* 虽然CCLP主要关注半监督情况，但在完全监督情况下，正则化器几乎完全减少到损失公式。
	* 重要的实际差异包括将对比嵌入归一化到单位球上，调整对比物镜中的温度参数，以及更强的增强。
	* Kamnitsas等人使用对比嵌入作为分类头的输入，该分类头与CCLP正则化器联合训练，而SupCon采用3个两阶段训练并丢弃对比头。
# 3 Method
* 给定一个输入批数据，首先应用两次数据增强以获得该批数据的两个副本；
* 两个副本通过编码器网络前向传播，以获得2048维规范化嵌入；
* 在训练期间，该表示通过投影网络进一步传播，该网络在推理时被丢弃，在投影网络的输出上计算监督对比损失；
* 为了使用训练好的模型进行分类，使用交叉熵损失在冻结表示的顶部训练一个线性分类器。
## 3.1 Representation Learning Framework
* 框架的主要组件：
	* Data Augmentation module
	* Encoder Network
	* Projection Network
## 3.2 Contrastive Loss Functions
* N个随机抽样的样本/标签对的集合：$\{x_k, y_k\}_{k=1...N}$
* 用于训练的相应批次由2N对组成:${ ̃x,  ̃y}_{l=1...2N}$
### 3.2.1 Self-Supervised Contrastive Loss
* 在一个多视图批处理中，设$i∈I≡{1…2N}$为任意增广样本的索引，设$j(i)$为来自同一源样本的另一个增广样本的索引
* 在自监督对比学习中，损失表现为以下形式:
	* ![[1701760644748.png]]
	* 索引$i$称为锚，索引$j(i)$称为正样本，其他$2(N−1)$个索引({k ∈ A(i) \ {j(i)})称为负样本。对于每个锚i，有1对正对和2N−2对负对。分母共有2N−1项(正负)。
### 3.2.2 Supervised Contrastive Losses
* ![[1701761728801.png]]
* 在(2)中，对正的求和位于对数的外部，而在(3)中，求和位于对数的内部。这两种损耗都有以下可取的性质:
	* Generalization to an arbitrary number of positives
		* 对于任何锚点，多视图批次(即，基于扩增的样本以及具有相同标签的任何剩余样本)中的所有正样本都有助于分子。
		* 对于随机生成的批，其大小相对于类的数量很大，将出现多个附加项(平均为N/C，其中C是类的数量)。
		* 监督损失鼓励编码器对来自同一类的所有条目给出紧密对齐的表示，从而产生比生成的表示空间更健壮的聚类。
	* Contrastive power increases with more negatives
		* 式2和式3都保留了式1的对比分母中负号的总和。 这种形式主要是由噪声对比估计和n对损失驱动的，其中，区分信号和噪声(负片)的能力是通过增加更多的负片的例子来提高的这一特性对于通过自监督对比学习进行表征学习是很重要的，许多论文显示，随着负样本的增加，性能也会提高。
	* Intrinsic ability to perform hard positive/negative mining
		* 当与规范化表示一起使用时，式1中的损失会产生一个梯度结构，从而产生隐式的硬正/负挖掘。硬阳性/阴性(即，继续对比锚对编码器大有好处)的梯度贡献很大，而简单阳性/阴性(即，继续对比锚对编码器只有微弱好处)的梯度贡献很小。对于硬阳性，效果随着阴性数量的增加而(渐近地)增加。式2和式3都保留了这个有用的性质，并将其推广到所有正数。这种隐式属性允许对比损失避开显式硬挖掘的需要，这是许多损失的微妙但关键的部分，例如三重损失[42]。我们注意到这个隐式性质适用于监督和自监督对比损失，但我们的推导是第一个清楚地表明这个性质
* 然而，这两种损失公式并不等效。由于log是凹函数，Jensen不等式意味着$L^{sup}_{in}≤L^{sup}_{out}$。因此，可以期望$L^{sup}_{out}$是更优的监督损失函数(因为它的上界是$L^{sup}_{in}$)。对于$L^{sup}_{out}$，正极归一化因子(即$1/|P (i)|$)用于消除多视图批中导致损失的正极中存在的偏差,尽管$L^{sup}_{in}$也包含相同的归一化因子，但它位于日志内部。因此，它只对总损失贡献一个附加常数，而不影响梯度。在没有任何归一化效果的情况下，$L^{sup}_{in}$的梯度更容易受到正值偏差的影响，从而导致次优训练.
# 4 Experiments
* 作者在评估其框架性能时，使用了**Top-1精度**和**对损坏图像的鲁棒性**两个方面进行衡量，还评价了其模型**对超参数的稳定性**以及**正对数量**对模型表现的影响。在实现上，使用的是训练好的网络，之后将网络的非线性投影头替换成一个简单的线性全连接层，使用标准交叉熵损失训练这个线性层。网络的训练在ImageNet上进行
## 4.1 Classification Accuracy
* 这部分实验比较了他们的方法与其他使用交叉熵的有监督方法的Top-1与Top-5精度，同时对比了他们的架构使用交叉熵损失的表现，可以看到，综合来说他们的方法实现了最好的效果，同时，他们的架构在使用交叉熵损失时的表现就不是非常好
## 4.2 Robustness to Image Corruptions and Reduced Training Data
* 这部分实验评价了他们的方法对图像扰动的稳定性，具体来说，他们选择使用对ImageNet数据库中的图像应用常见的自然扰动，比如加噪声、模糊和对比度变化，构造得到的ImageNet-c数据集进行测试。使用平均损坏误差与平均相对损坏误差作为评价指标，可以看到，他们的方法的误差最小，且使用改进的对比损失替换交叉熵损失也有助于提升网络的性能。此外，和交叉熵损失相比，本文的对比损失在不同程度的图像损坏下都能保持一个相对稳定的平均损失误差，相比于交叉熵损失也有更高的Top-1精度.
## 4.3 Hyperparameter Stability
* 通常深度网络对超参数都很敏感，本文还比较了他们的改进对比损失对不同优化器、不同数据增强和学习率的分类精度稳定性。三种增强方式是本文提出的三种；优化器则选用了LARS、带动量的SGD和RMSProp；选择了最佳学习率以及增大或减小十倍的三个学习率进行评估，可以发现，本文提出的loss确实和交叉熵损失相比，对这三种超参数的变化更鲁棒。
## 4.4 Transfer Learning
* SupCon与交叉熵和自监督对比损失在相同架构上训练时的迁移学习性能相当虽然更好的ImageNet模型与更好的传输性能相关，但主要因素是架构。
## 4.5 Training Details
* 在ResNet-200预训练中，SupCon损失训练了700个epoch，在较小的模型中训练了350个epoch。显示了ResNet50的准确性作为SupCon训练epoch的函数，表明即使200个epoch对于大多数目的也可能足够了。训练线性分类器的一个(可选的)附加步骤用于计算top-1精度。如果目的是将表示用于迁移学习任务或检索，则不需要这样做。第二阶段只需要10个阶段的额外训练。请注意，在实践中，线性分类器可以通过阻止梯度传播从线性分类器返回到编码器来与编码器和投影网络联合训练，并且无需两阶段训练即可获得大致相同的结果
* 所有的结果都使用τ = 0.1的温度。较低的温度比较高的温度更有利于训练，但由于数值不稳定，极低的温度更难训练。图4(d)显示了温度对有监督对比学习Top-1性能的影响。梯度随温度τ的选择呈反比.