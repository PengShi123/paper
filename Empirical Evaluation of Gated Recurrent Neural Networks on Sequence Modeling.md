# GRU VS LSTM

## Abstract
* 文章主要阐述的问题是：在本文中，比较循环神经网络 (RNN) 中不同类型的循环单元，在复调音乐建模和语音信号建模任务上，两者在于传统的tanh单元进行比较
## 1、Introduction
* 现有的循环神经网络，都是一个具有复杂循环隐藏单元的循环神经网络
* LSTM 单元在具有长期依赖关系的基于序列的任务上运行良好的领域中得到了很好的建立
* GRU被引入并用于机器翻译的背景下
* 数据集：三个复调音乐数据集，两个内部数据集，其中每个样本都是原始语音表示。
* 结论：通过在某些数据集 GRU 上使用固定数量的参数，在 CPU 时间和参数更新和泛化方面均优于 LSTM 单元。
## 2 Background: Recurrent Neural Network
* 循环神经网络 (RNN) 是传统前馈神经网络的扩展，能够处理可变长度的序列输入
* 通过一个循环隐藏状态，其每次激活都依赖于前一个时间的激活。
* 其中φ是一个非线性函数
* 其中 g 是一个平滑的有界函数，例如逻辑 sigmoid 函数或双曲正切函数。
* 生成 RNN 在其当前状态 ht 的情况下输出序列下一个元素的概率分布，并且该生成模型可以通过使用特殊输出符号来表示序列的末尾来捕获可变长度的序列上的分布
* 最后一个元素是一个特殊的序列结束值。
* 梯度趋向于消失(大部分时间)或爆炸(很少，但影响严重)
* 梯度爆炸：由于长期依赖的影响被短期依赖的影响隐藏（相对于序列长度呈指数增长）。
* 解决方法：
  * 设计一个比简单的随机梯度下降更好的学习算法：剪切梯度，使用二阶方法
  * 由仿射变换和简单的元素非线性组成，通过使用门控单元。
  * 使用这些循环单元中的任何一个的 RNN 已被证明在需要捕获长期依赖关系的任务中表现良好。
* (a) LSTM 和 (b) 门控循环单元的图示。(a) i, f 和 o 分别是输入门、遗忘门和输出门。c 和 ̃c 表示存储单元和新存储单元内容。(b)r 和 z 是重置和更新门，h 和 ̃h 是激活和候选激活。 
## 3 Gated Recurrent Neural Networks
* 每个第 j 个 LSTM 单元在时间 t 维护一个内存 cj t。LSTM 单元的输出
### 3.1 Long Short-Term Memory Unit
* σ 是一个逻辑 sigmoid 函数。Vo 是一个对角矩阵。
* 存储单元cj t通过部分忘记现有的内存并添加一个新的内存内容̃cj t来更新
* 现有的记忆被遗忘的程度由遗忘门 fjt 调制
* 与传统的循环单元在每个时间步覆盖其内容不同，LSTM 单元能够通过引入的门决定是否保留现有内存。
* 直观地说，如果 LSTM 单元在早期阶段从输入序列中检测到重要特征，则很容易在长距离上携带此信息，从而捕获潜在的长距离依赖关系。
### 3.2 Gated Recurrent Unit
* Cho等人[2014]提出了一种门控循环单元(GRU)，使每个循环单元自适应地捕获不同时间尺度的依赖关系
* 没有单独的存储单元
* 前一个激活hj t−1与候选激活̃hj t之间的线性插值
* 其中更新门zj t决定单元更新其激活程度或内容
* GRU 没有任何机制来控制其状态暴露的程度，但每次暴露整个状态。
* 当关闭（rjt 接近 0）时，重置门有效地使单元充当读取输入序列的第一个符号，从而允许它忘记先前计算的状态。
### 3.3 Discussion
* 传统的循环单元总是用从当前输入和前一个隐藏状态计算的新值替换单元的激活或内容。另一方面，LSTM 单元和 GRU 都保留了现有内容并在其之上添加新内容
* 首先，每个单元很容易记住输入流中特定特征对于长序列步骤的存在。
* 其次更重要的是，这种添加有效地创建了绕过多个时间步长的快捷路径，这些快捷方式允许错误很容易反向传播，而不会太快地消失（如果门控单元在 1 处几乎饱和），这是由于通过多个有界非线性，从而降低了梯度消失造成的困难
* GRU 中缺少的 LSTM 单元的一个特征是内存内容的受控曝光。
* 在 LSTM 单元中，网络中其他单元看到的内存内容的数量由输出门控制。另一方面，GRU 在没有任何控制的情况下暴露了其完整内容
* LSTM 单元计算新的内存内容，而无需对从前一个时间步流出的信息量进行任何单独的控制
* 另一方面，GRU 在计算新的候选激活时控制来自先前激活的信息流，但不独立控制正在添加的候选激活的数量（控制通过更新门绑定）。
## 4 Experiments Setting
### 4.1 Tasks and Datasets
* 序列建模旨在学习序列上的概率分布
* 数据集：
  * Nottingham
  * JSB Chorales
  * MuseData 
  * Piano-midi
* 使用逻辑回归作为输出单元
* 查看20个连续样本来预测接下来的10个连续样本。
### 4.2 Models
* 每个都有 LSTM 单元、GRU、tanh 单元
* RMSProp训练每个模型，并使用标准差固定为0.075的权重噪声
## 5 Results and Analysis
* 在复调音乐数据集的情况下，除了诺丁汉之外，GRU-RNN 在所有数据集上的表现优于所有其他（LSTM-RNN 和 tanh-RNN）
* GRU-RNN 在 6 次更新和实际 CPU 时间方面都取得了更快的进展。
* 如果我们考虑 Ubisoft 数据集，很明显，尽管 tanh-RNN 中每次更新的计算要求比其他模型小得多，但它并没有在每次更新方面取得太多进展，最终停止在更差的水平上取得任何进展。
* 收敛通常更快，最终解决方案往往更好。
* 作者认为影响效果的是在数据集和相应任务
## 6 Conclusion
* 评估清楚地证明了门控单元的优越性； LSTM 单元和 GRU 都优于传统的 tanh 单元。这在更具挑战性的原始语音信号建模任务中更为明显